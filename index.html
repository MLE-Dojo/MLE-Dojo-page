<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <!-- <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <!-- <meta property="og:image" content="static/images/carousel2.jpg" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/> --> 


  <!-- <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <!-- <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <!-- <meta name="keywords" content="LLMs, Adaptation">
  <meta name="viewport" content="width=device-width, initial-scale=1"> --> 


  <title>MLE-Dojo</title>
  <link rel="icon" type="image/x-icon" href="static/images/scp/square_new.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <figure style="text-align: center; margin-bottom: 1rem;">
              <img src="static/images/scp/icon.jpg" alt="MLE-Dojo Logo" style="width: 300px; height: auto;">
            </figure>
          <h1 class="title is-1 publication-title" style="font-size: 2.2em; font-weight: normal;"> <strong style="font-weight: bold; font-style: monospace;">MLE-Dojo</strong>: <span style="font-weight: normal;">Improving LLM Agents for Machine Learning Engineering in Interactive Environments</span> </h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://rushi-q.github.io/" target="_blank">Rushi Qiang</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="https://night-chen.github.io" target="_blank">Yuchen Zhuang</a><sup>*</sup>,</span>
                  <span class="author-block">
                    <a href="https://yinghao-li.github.io/e" target="_blank">Yinghao Li</a><sup></sup>,</span>
                    <span class="author-block">
                      <a href="" target="_blank">Dingu Sagar V K</a><sup></sup>,</span>
                  <span class="author-block">
                    <a href="https://rongzhizhang.org/" target="_blank">Rongzhi Zhang</a><sup></sup>,</span>
                  <span class="author-block">
                    <a href="https://lichangh20.github.io/" target="_blank">Changhao Li</a><sup></sup>,</span>
                  <span class="author-block">
                    <a href="" target="_blank">Ian Shu-Hei Wong</a><sup></sup>,</span>
                  <span class="author-block">
                    <a href="https://sherryy.github.io/" target="_blank">Sherry Yang</a><sup></sup>,</span>
                  <span class="author-block">
                    <a href="https://cs.stanford.edu/~pliang/" target="_blank">Percy Liang</a><sup></sup>,</span>
                  <span class="author-block">
                    <a href="http://chaozhang.org/" target="_blank">Chao Zhang</a><sup></sup>,</span>
                  <span class="author-block">
                    <a href="https://bo-dai.github.io/" target="_blank">Bo Dai</a><sup></sup></span>
                    
                  </span> 

                  
                  </div>

              <div class="is-size-5 publication-authors">
                <span class="author-block">Georgia Institute of Technology</span>&nbsp;&nbsp;&nbsp;
                <span class="author-block">Stanford University</span>
                <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
              </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://mle-dojo.github.io/MLE-Dojo-page/" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/MLE-Dojo/MLE-Dojo" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://mle-dojo.github.io/MLE-Dojo-page/" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>



                <!-- Hugging Face Leaderboard Link -->
                <span class="link-block">
                  <a href="https://huggingface.co/spaces/MLE-Dojo/Leaderboard" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-chart-bar"></i>
                    </span>
                    <span>Leaderboard</span>
                  </a>
                </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
        <!-- Your video here -->
        <img src="static/images/scp/overview.jpg" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        MLE-Dojo is a Gym-style framework for systematically training, evaluating, and improving large language model (LLM) agents in iterative machine learning engineering (MLE) workflows.</h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We introduce MLE-Dojo, a Gym-style framework for systematically training, evaluating, and improving autonomous large language model (LLM) agents in iterative machine learning engineering (MLE) workflows. 
Unlike existing benchmarks that primarily rely on static datasets or single-attempt evaluations, MLE-Dojo provides an interactive environment enabling agents to iteratively experiment, debug, and refine solutions through structured feedback loops. 
Built upon 200+ real-world Kaggle challenges (\eg, tabular data analysis, computer vision, natural language processing, and time series forecasting). MLE-Dojo covers diverse, open-ended MLE tasks carefully curated to reflect realistic engineering scenarios such as data processing, architecture search, hyperparameter tuning, and code debugging.
Its fully executable environment supports comprehensive agent training via both supervised fine-tuning and reinforcement learning, facilitating iterative experimentation, realistic data sampling, and real-time outcome verification. 
Extensive evaluations of eight frontier LLMs reveal that while current models achieve meaningful iterative improvements, they still exhibit significant limitations in autonomously generating long-horizon solutions and efficiently resolving complex errors. 
Furthermore, MLE-Dojo's flexible and extensible architecture seamlessly integrates diverse data sources, tools, and evaluation protocols, uniquely enabling model-based agent tuning and promoting interoperability, scalability, and reproducibility.          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->




<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">Introduction</h2>
    MLE-Dojo serves as a systematic framework for training, evaluating, and improving MLE agents, with four key features:
    <ul>
      <li><b>Comprehensive Framework and Benchmark:</b> We establish MLE-Dojo as a comprehensive and large-scale benchmark consisting of over 200 Kaggle MLE competitions, enabling systematic and rigorous evaluations of autonomous LLM agents.</li>
      <li><b>Interactive and Executable Environment:</b> MLE-Dojo provides an interactive and fully executable Gym-style environment that facilitates iterative experimentation, including comprehensive training trajectories sampling for supervised fine-tuning and reinforcement learning.</li>
      <li><b>Advanced Functionalities and Scalability Supports:</b> MLE-Dojo uniquely facilitates outcome verification, model-agnostic agent tuning, and seamless integration of diverse datasets and tools, significantly accelerating the development of robust, generalizable, and scalable MLE agents.</li>
      <li><b>Extensive Empirical Evaluation and Public Leaderboard:</b> We conduct large-scale evaluations across multiple state-of-the-art LLMs and agent scaffolds, with results publicly available through an actively maintained long-term real-time leaderboard to foster community-driven innovation.</li>
    </ul>
    <br><br>
    <div style="display: flex; justify-content: center;">
      <img src="static/images/scp/elo_bar.jpg" alt="MY ALT TEXT"width="720" height="480">
  </div> 
    <h2 class="title">Data</h2>
    <p>
    The <strong>MLE-Dojo</strong> benchmark comprises over <strong>200 real-world machine learning tasks</strong> spanning <em>tabular data, computer vision, NLP, and time series</em>, sourced from Kaggle. Each task is <strong>standardized</strong> into a unified format—featuring structured descriptions, reorganized datasets, local evaluators, and human leaderboards—designed for seamless interaction with LLM agents. Tasks are selected for their diversity, practical relevance, and validation feasibility, forming a <strong>scalable and extensible dataset</strong> tailored for training and evaluating autonomous ML agents under realistic, iterative workflows.
    Users can effortlessly and flexibly incorporate new tasks, enabling seamless adaptation to diverse requirements and application scenarios.
    </p>
    <div style="text-align: center; margin-top: 1.5em;">
    <img src="static/images/scp/data.jpg" alt="MY ALT TEXT" style="width: 720px; height: auto;">
    </div>
    <h2 class="title">Key Features</h2>

<!--     <p><strong>Overview:</strong> <code>MLE-Dojo</code> provides a unified, Gym-style framework for training and evaluating LLM-based MLE agents in realistic, iterative ML engineering environments. It formalizes interactions as POMDPs and supports structured feedback through code execution, observation, and reward signals.</p> -->
    
    <p><strong>Modular and User-Friendly Interface:</strong> The environment is composed of modular components—<code>Error</code>, <code>Interface</code>, <code>Feedback</code>, and <code>Metric</code>—that are fully decoupled and extensible via a clean registration API. A single <code>env.step</code> call enables seamless agent-environment interaction, simplifying agent design and integration.</p>
    
    <p><strong>Extensible Task Space:</strong> All tasks are isolated in reproducible Docker containers with configurable execution sandboxes. A unified data format standardizes integration, allowing users to add custom competitions with minimal effort, ensuring compatibility and secure agent testing.</p>
    
    <p><strong>Observation Space:</strong> Each environment provides rich, structured observations, including competition context, evaluation metrics, code execution results, detailed error messages, and both agent- and environment-side interaction histories. This empowers agents with full situational awareness.</p>
    
    <p><strong>Expandable Action Space:</strong> <code>MLE-Dojo</code> supports five core actions—<code>request_info</code>, <code>validate_code</code>, <code>execute_code</code>, <code>get_history</code>, and <code>reset</code>—and allows users to register new actions through a customizable portal, enabling advanced experimentation and behavior design.</p>
    
    <p><strong>Reward Space and Environmental Feedback:</strong> Instead of coarse-grained medals, <code>MLE-Dojo</code> uses the <strong>HumanRank Score</strong>, a normalized leaderboard-based reward that reflects how well agents perform compared to human participants. This enables unified, fine-grained evaluation across diverse competitions.</p>

    <h2 class="title">Experiments</h2>
    We evaluate 8 mainstream LLMs as MLE Agents on 50 MLE-Dojo Evaluation tasks, which covers Tabular, NLP, CV, Time Series and so on.
    To ensure a comprehensive evaluation, we consider Area Under the Performance Profile (AUP), HumanRank Score (H-Rank, %), and Elo ranking together as metrics. 
    We actively maintain a long-term real-time leaderboard to foster community-driven innovation.
    <h3 class="title">Main Results</h3>
    Reasoning and coding models such as <code>o3-mini</code>, <code>DeepSeek-r1</code>, and <code>Gemini-2.5-Pro</code> 
  consistently achieve high rankings across all metrics, demonstrating strong adaptability, robustness, and overall effectiveness as MLE Agents.
    <img src="static/images/scp/table.png" alt="MY ALT TEXT">
    <h3 class="title">Difficulty</h3>
    We define the difficulty level of different tasks with the average performance of different
models in comparison with the human leaderboard. As shown in the figure, CV tasks are the most
challenging-none of them have an average HumanRank score above 60, and more than half fall below
30. For MLE-Lite tasks, the average HumanRank scores mostly exceed 30. Difficulty levels of tasks
in other domains are more evenly distributed. 
    <br><br>
    <img src="static/images/scp/difficulty.jpg" alt="MY ALT TEXT">
<!--     <h3 class="title">Cost v.s. Performance</h3>
    Figure below illustrates the cost-performance relationship across different LLMs and task categories.

    Reasoning models (<em>e.g.</em>, <code>DeepSeek-r1</code>) typically incur higher costs due to their premium pricing structures and longer solution outputs.

    Even reasoning models with comparatively lower pricing, such as <code>o3-mini</code>, tend to produce longer outputs due to more complex reasoning processes. These longer outputs significantly increase overall token consumption, contributing to higher cumulative costs.

    Notably, tasks involving computer vision and deep neural network training pipelines consistently generate longer codes compared to classical ML tasks (<em>e.g.</em>, tabular analysis) executed on CPUs.

    While cost generally correlates with solution complexity and token consumption, some models, such as <code>DeepSeek-r1</code>, achieve competitive performance with significantly fewer tokens, highlighting potential cost-efficiency opportunities.
    <div style="height: 1.5em;"></div> -->
<!--     <img src="static/images/scp/cost.jpg" alt="MY ALT TEXT">
    <h3 class="title">Step-wise Performance Dynamics</h3>
    Figure below presents the step-wise performance improvements across different models, illustrating variations in performance 
    trajectories between reasoning and non-reasoning models. Among reasoning models, <code>o3-mini</code> consistently achieves high 
    performance within the initial steps (typically within the first five) and maintains stable scores in subsequent steps. Conversely, 
    <code>DeepSeek-r1</code> and <code>Gemini-2.5-Pro</code> exhibit gradual improvements, achieving comparable or superior performance 
    in intermediate to later steps. Non-reasoning models occasionally outperform reasoning models at early or intermediate steps but 
    generally show limited improvement as steps progress, resulting in lower final scores.
    <div style="height: 1.5em;"></div>
    <img src="static/images/scp/improve.jpg" alt="MY ALT TEXT">
    <h3 class="title">History Length and Solution Length</h3>
    Figure below shows both the total chat history length, including all interaction prompts, 
    actions, and generated codes, and the length of the best solution generated by each model. 
    The total chat history length closely aligns with the best solution length, where both metrics 
    positively correlate with overall model performance. Reasoning models typically generate notably  -->
<!--     longer solutions compared to non-reasoning models. Moreover, stronger-performing models frequently 
    produce more extended solutions, which often correspond to higher performance scores. Although 
    increased solution length does not inherently ensure superior outcomes, it generally indicates a 
    model's capability to explore more intricate and sophisticated solution strategies, a hallmark 
    predominantly observed in more capable reasoning models.
    <div style="text-align: center; margin-top: 1.5em;">
    <img src="static/images/scp/history.jpg" alt="MY ALT TEXT" style="width: 600px; height: auto;">
    </div> -->
    <h2 class="title">Develop with MLE-Dojo</h2>
    <p>
    MLE-Dojo provides flexible Gym-style APIs that allow users to build personalized environments, introduce new datasets and develop/utilize different agent scaffolds. 
    To facilitate model training via Supervised Fine-Tuning (SFT) or Reinforcement Learning (RL), MLE-Dojo provides a detailed history management system and a well-defined reward feedback mechanism, including both final outcome rewards and intermediate step rewards.
      
    </p>
    <h3 class="subtitle">Interface and APIs</h3>
    <p>
      <a href="https://github.com/MLE-Dojo/MLE-Dojo/blob/main/example/example.ipynb" target="_blank">
        📘 Quick API Example
      </a>
    </p>
    <p>
      <strong>MLE-Dojo</strong> offers a modular, Gym-style API for building personalized ML environments, integrating datasets, and customizing agent scaffolds. The toolkit supports intuitive interaction with ML competitions through the following key components:
    </p>
    
    <ul>
      <li>
        <strong>Interface</strong>: Central hub to interact with competitions. Includes:
        <ul>
          <li><strong>InfoInterface</strong>: Retrieves task metadata and dataset structure.</li>
          <li><strong>CodeValidationInterface</strong>: Performs safe, syntax-level checks in a sandbox.</li>
          <li><strong>CodeExecutionInterface</strong>: Executes user code, generates submissions, and triggers evaluations.</li>
          <li><strong>Modular Design</strong>: Supports custom sub-interface registration for advanced workflows.</li>
        </ul>
      </li>
      <li>
        <strong>Sandbox</strong>: Runs user code securely in isolated environments with GPU/memory/time constraints.
      </li>
      <li>
        <strong>FeedbackManager</strong>: Processes outputs and delivers structured feedback. Easily extendable via plug-in feedback providers.
      </li>
      <li>
        <strong>KaggleEnvironment</strong>: Wraps all components into a Gym-compatible interface. Manages setup, action execution, state tracking, and feedback generation.
      </li>
      <li>
        <strong>KaggleEnvironment</strong>: Wraps all components into a Gym-compatible interface. Manages setup, action execution, state tracking, and feedback generation.
      </li>
    </ul>
    
    <p>
      Together, these modules ensure flexible development and secure execution, offering a unified and extensible system for LLM-agent research.
    </p>
    
    <h3 class="subtitle">Collect Trajectories for Training</h3>
    <p>
      MLE-Dojo tracks both <strong>agent behavior</strong> and <strong>environment responses</strong> with structured feedback, enabling training via Supervised Fine-Tuning (SFT) or Reinforcement Learning (RL).
      The structured trajectory samples can be found at
<a href="https://github.com/MLE-Dojo/MLE-Dojo/blob/main/trajectories/agent_trajectory.json" target="_blank"><strong>Agent History</strong></a> and
<a href="https://github.com/MLE-Dojo/MLE-Dojo/blob/main/trajectories/env_history.json" target="_blank"><strong>Environment History</strong></a>.
    </p>
      
  </div>
</section>












<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code></code></pre>
    </div>
</section>
<!--End BibTex citation -->


 
  </body>
  </html>
