<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <!-- <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <!-- <meta property="og:image" content="static/images/carousel2.jpg" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/> --> 


  <!-- <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <!-- <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <!-- <meta name="keywords" content="LLMs, Adaptation">
  <meta name="viewport" content="width=device-width, initial-scale=1"> --> 


  <title>MLE-Dojo</title>
  <link rel="icon" type="image/x-icon" href="static/images/scp/square_new.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <figure style="text-align: center; margin-bottom: 1rem;">
              <img src="static/images/scp/icon.jpg" alt="MLE-Dojo Logo" style="width: 300px; height: auto;">
            </figure>
            <h1 class="title is-1 publication-title" style="font-size: 2.9em; font-weight: bold;">MLE-Dojo<br>
              <span style="font-size: 0.8em; font-weight: normal;">Improving LLM Agents for Machine
              Learning Engineering in Interactive Environments</span></h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://haotiansun.tech" target="_blank">Haotian Sun</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="https://night-chen.github.io" target="_blank">Yuchen Zhuang</a><sup>*</sup>,</span>
                  <span class="author-block">
                    <a href="http://www.weiwei.one" target="_blank">Wei Wei</a><sup></sup>,</span>
                    <span class="author-block">
                      <a href="http://chaozhang.org" target="_blank">Chao Zhang</a><sup></sup>,</span>
                  <span class="author-block">
                    <a href="https://bo-dai.github.io" target="_blank">Bo Dai</a>
                  </span>
                  
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Georgia Institute of Technology</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2402.08219.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/jerrycool2002/MLE-Dojo" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2402.08219" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
        <!-- Your video here -->
        <img src="static/images/scp/overview.jpg" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        MLE-Dojo is a Gym-style framework for systematically training, evaluating, and improving large language model (LLM) agents in iterative machine learning engineering (MLE) workflows.</h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We introduce MLE-Dojo, a Gym-style framework for systematically training, evaluating, and improving autonomous large language model (LLM) agents in iterative machine learning engineering (MLE) workflows. 
Unlike existing benchmarks that primarily rely on static datasets or single-attempt evaluations, MLE-Dojo provides an interactive environment enabling agents to iteratively experiment, debug, and refine solutions through structured feedback loops. 
Built upon 200+ real-world Kaggle challenges (\eg, tabular data analysis, computer vision, natural language processing, and time series forecasting). MLE-Dojo covers diverse, open-ended MLE tasks carefully curated to reflect realistic engineering scenarios such as data processing, architecture search, hyperparameter tuning, and code debugging.
Its fully executable environment supports comprehensive agent training via both supervised fine-tuning and reinforcement learning, facilitating iterative experimentation, realistic data sampling, and real-time outcome verification. 
Extensive evaluations of eight frontier LLMs reveal that while current models achieve meaningful iterative improvements, they still exhibit significant limitations in autonomously generating long-horizon solutions and efficiently resolving complex errors. 
Furthermore, MLE-Dojo's flexible and extensible architecture seamlessly integrates diverse data sources, tools, and evaluation protocols, uniquely enabling model-based agent tuning and promoting interoperability, scalability, and reproducibility.          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->




<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">Introduction</h2>
    MLE-Dojo serves as a systematic framework for training, evaluating, and improving MLE agents, with four key features:
    <ul>
      <li><b>Comprehensive Framework and Benchmark:</b> We establish MLE-Dojo as a comprehensive and large-scale benchmark consisting of over 200 Kaggle MLE competitions, enabling systematic and rigorous evaluations of autonomous LLM agents.</li>
      <li><b>Interactive and Executable Environment:</b> MLE-Dojo provides an interactive and fully executable Gym-style environment that facilitates iterative experimentation, including comprehensive training trajectories sampling for supervised fine-tuning and reinforcement learning.</li>
      <li><b>Advanced Functionalities and Scalability Supports:</b> MLE-Dojo uniquely facilitates outcome verification, model-agnostic agent tuning, and seamless integration of diverse datasets and tools, significantly accelerating the development of robust, generalizable, and scalable MLE agents.</li>
      <li><b>Extensive Empirical Evaluation and Public Leaderboard:</b> We conduct large-scale evaluations across multiple state-of-the-art LLMs and agent scaffolds, with results publicly available through an actively maintained long-term real-time leaderboard to foster community-driven innovation.</li>
    </ul>
    <br><br>
    <div style="display: flex; justify-content: center;">
      <img src="static/images/scp/elo_bar.jpg" alt="MY ALT TEXT"width="720" height="480">
  </div>  
    <h2 class="title">Experiments</h2>
    We evaluate 8 mainstream LLMs as MLE Agents on 50 MLE-Dojo Evaluation tasks, which covers Tabular, NLP, CV, Time Series and so on.
    To ensure a comprehensive evaluation, we consider Area Under the Performance Profile (AUP), HumanRank Score (H-Rank, %), and Elo ranking together as metrics. 
    We actively maintain a long-term real-time leaderboard to foster community-driven innovation.<ul>
    <h3 class="title">Main Results</h3>
    Reasoning and coding models such as <code>o3-mini</code>, <code>DeepSeek-r1</code>, and <code>Gemini-2.5-Pro</code> 
  consistently achieve high rankings across all metrics, demonstrating strong adaptability, robustness, and overall effectiveness as MLE Agents.
    <img src="static/images/scp/table.png" alt="MY ALT TEXT">
    <h3 class="title">Difficulty</h3>
    We define the difficulty level of different tasks with the average performance of different
models in comparison with the human leaderboard. Figure below illustrates the average performance
distribution across 8 frontier models on the tasks. As shown in the figure, CV tasks are the most
challenging-none of them have an average HumanRank score above 60, and more than half fall below
30. For MLE-Lite tasks, the average HumanRank scores mostly exceed 30. Difficulty levels of tasks
in other domains are more evenly distributed. <br>
      <img src="static/images/scp/difficulty.jpg" alt="MY ALT TEXT">
    <h3 class="title">Cost v.s. Performance</h3>
    Figure below illustrates the cost-performance relationship across different LLMs and task categories.

    Reasoning models (<em>e.g.</em>, <code>DeepSeek-r1</code>) typically incur higher costs due to their premium pricing structures and longer solution outputs.

    Even reasoning models with comparatively lower pricing, such as <code>o3-mini</code>, tend to produce longer outputs due to more complex reasoning processes. These longer outputs significantly increase overall token consumption, contributing to higher cumulative costs.

    Notably, tasks involving computer vision and deep neural network training pipelines consistently generate longer codes compared to classical ML tasks (<em>e.g.</em>, tabular analysis) executed on CPUs.

    While cost generally correlates with solution complexity and token consumption, some models, such as <code>DeepSeek-r1</code>, achieve competitive performance with significantly fewer tokens, highlighting potential cost-efficiency opportunities.

    <img src="static/images/scp/cost.jpg" alt="MY ALT TEXT">
    <h3 class="title">Step-wise Performance Dynamics</h3>
    Figure below presents the step-wise performance improvements across different models, illustrating variations in performance 
    trajectories between reasoning and non-reasoning models. Among reasoning models, <code>o3-mini</code> consistently achieves high 
    performance within the initial steps (typically within the first five) and maintains stable scores in subsequent steps. Conversely, 
    <code>DeepSeek-r1</code> and <code>Gemini-2.5-Pro</code> exhibit gradual improvements, achieving comparable or superior performance 
    in intermediate to later steps. Non-reasoning models occasionally outperform reasoning models at early or intermediate steps but 
    generally show limited improvement as steps progress, resulting in lower final scores.
    <br>
    <img src="static/images/scp/improve.jpg" alt="MY ALT TEXT">
    <h3 class="title">History Length and Solution Length</h3>
    Figure below shows both the total chat history length, including all interaction prompts, 
    actions, and generated codes, and the length of the best solution generated by each model. 
    The total chat history length closely aligns with the best solution length, where both metrics 
    positively correlate with overall model performance. Reasoning models typically generate notably 
    longer solutions compared to non-reasoning models. Moreover, stronger-performing models frequently 
    produce more extended solutions, which often correspond to higher performance scores. Although 
    increased solution length does not inherently ensure superior outcomes, it generally indicates a 
    model's capability to explore more intricate and sophisticated solution strategies, a hallmark 
    predominantly observed in more capable reasoning models.
    <br>
    <img src="static/images/scp/history.jpg" alt="MY ALT TEXT">

  </div>
</section>












<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{sun2024bboxadapter,
        title={BBox-Adapter: Lightweight Adapting for Black-Box Large Language Models}, 
        author={Haotian Sun and Yuchen Zhuang and Wei Wei and Chao Zhang and Bo Dai},
        year={2024},
        eprint={2402.08219},
        archivePrefix={arXiv},
        primaryClass={cs.CL}
  }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


 
  </body>
  </html>
